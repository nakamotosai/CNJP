import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time
import requests
from bs4 import BeautifulSoup
import re

# è®¾ç½®æ—¶åŒº UTC+9ï¼ˆæ—¥æœ¬æ—¶é—´ï¼‰
JST_OFFSET = datetime.timedelta(hours=9)

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

def extract_image(entry):
    # å°è¯•ä» Google News çš„ summary/description ä¸­æå–å›¾ç‰‡
    content_html = ""
    if 'summary' in entry:
        content_html = entry.summary
    elif 'description' in entry:
        content_html = entry.description

    if content_html:
        try:
            soup = BeautifulSoup(content_html, 'html.parser')
            img = soup.find('img')
            if img and 'src' in img.attrs:
                return img['src']
        except:
            pass
    return ""

def classify_news(title):
    # ç®€å•çš„å…³é”®è¯åˆ†ç±»
    keywords = {
        "æ—¶æ”¿": ["æ”¿åºœ", "æ”¿ç­–", "ä¹ è¿‘å¹³", "æå¼º", "å¤–äº¤", "æ”¿æ²»", "é€‰ä¸¾", "è®®å‘˜", "é¦–ç›¸", "æ€»ç»Ÿ", "å†›äº‹", "å›½é˜²", "ä¸­å…±", "å…š", "å°æ¹¾", "é¦™æ¸¯", "äººæƒ", "åˆ¶è£", "å¤§ä½¿", "é¢†äº‹", "æ¡çº¦", "åå®š", "å³°ä¼š", "ä¼šè°ˆ", "å¤§è‡£", "å†…é˜", "å›½ä¼š", "å‚è®®é™¢", "ä¼—è®®é™¢", "è‡ªæ°‘å…š", "å…¬æ˜å…š", "ç«‹å®ªæ°‘ä¸»å…š", "ç»´æ–°ä¼š", "å…±äº§å…š", "å›½æ°‘æ°‘ä¸»å…š", "ä»¤å’Œæ–°é€‰ç»„", "ç¤¾æ°‘å…š", "å‚æ”¿å…š", "æ‹œç™»", "ç‰¹æœ—æ™®", "æ™®äº¬", "å²¸ç”°", "çŸ³ç ´", "é«˜å¸‚", "å°æ³‰", "æ²³é‡", "æ—èŠ³æ­£", "èŒ‚æœ¨", "åŠ è—¤", "ä¸Šå·", "æ ¸æ­¦å™¨", "å¯¼å¼¹", "æ¼”ä¹ ", "å·¡é€»", "æµ·è­¦", "é’“é±¼å²›", "å°–é˜", "å—æµ·", "ä¸œæµ·", "å°æµ·"],
        "ç»æµ": ["ç»æµ", "è´¸æ˜“", "è‚¡å¸‚", "æŠ•èµ„", "é“¶è¡Œ", "ä¼ä¸š", "GDP", "å¸‚åœº", "æ¶ˆè´¹", "äº§ä¸š", "æ±‡ç‡", "ç¾å…ƒ", "æ—¥å…ƒ", "ç”µåŠ¨è½¦", "EV", "åŠå¯¼ä½“", "èŠ¯ç‰‡", "é€šèƒ€", "ç‰©ä»·", "å·¥èµ„", "å°±ä¸š", "å¤±ä¸š", "æˆ¿åœ°äº§", "æ¥¼å¸‚", "å¤®è¡Œ", "åˆ©ç‡", "åŠ æ¯", "é™æ¯", "å…³ç¨", "å‡ºå£", "è¿›å£", "ä¾›åº”é“¾", "åˆ¶é€ ", "ä¸°ç”°", "æœ¬ç”°", "æ—¥äº§", "ç´¢å°¼", "æ¾ä¸‹", "è½¯é“¶", "ä¼˜è¡£åº“", "ä»»å¤©å ‚", "é˜¿é‡Œ", "è…¾è®¯", "å­—èŠ‚", "åä¸º", "æ¯”äºšè¿ª", "å®å¾·æ—¶ä»£", "è´¢æŠ¥", "äºæŸ", "ç›ˆåˆ©", "æ”¶è´­", "åˆå¹¶", "ç ´äº§", "è£å‘˜"],
        "ç¤¾ä¼š": ["ç¤¾ä¼š", "äººå£", "æ•™è‚²", "åŒ»ç–—", "çŠ¯ç½ª", "äº‹æ•…", "ç¾å®³", "ç–«æƒ…", "ç”Ÿæ´»", "æ—…æ¸¸", "ç­¾è¯", "ç§»æ°‘", "å°‘å­åŒ–", "è€é¾„åŒ–", "å…»è€", "ç¦åˆ©", "ä¿é™©", "åŒ»é™¢", "åŒ»ç”Ÿ", "æŠ¤å£«", "å­¦æ ¡", "å­¦ç”Ÿ", "è€å¸ˆ", "å¤§å­¦", "é«˜è€ƒ", "ç•™å­¦", "æ²»å®‰", "è­¦å¯Ÿ", "é€®æ•", "å®¡åˆ¤", "æ³•é™¢", "å¾‹å¸ˆ", "åœ°éœ‡", "å°é£", "æš´é›¨", "æ´ªæ°´", "ç«ç¾", "äº¤é€š", "é“è·¯", "æ–°å¹²çº¿", "èˆªç­", "æœºåœº", "åœ°é“", "å…¬äº¤", "å‡ºç§Ÿè½¦", "é£Ÿå“", "å®‰å…¨", "ç¯å¢ƒ", "æ±¡æŸ“", "åƒåœ¾", "æ°”å€™", "å˜æš–", "ç¢³ä¸­å’Œ", "æ ¸ç”µ", "æ ¸æ±¡æ°´", "æ’æµ·", "é–å›½ç¥ç¤¾", "ç†ŠçŒ«"],
        "ä½“è‚²": ["ä½“è‚²", "å¥¥è¿", "è¶³çƒ", "ç¯®çƒ", "æ£’çƒ", "é€‰æ‰‹", "æ¯”èµ›", "å† å†›", "å¤§è°·", "ç¿”å¹³", "ç¾½ç”Ÿ", "ç»“å¼¦", "ä¹’ä¹“", "ç½‘çƒ", "æ¸¸æ³³", "ç”°å¾„", "é©¬æ‹‰æ¾", "ç›¸æ‰‘", "æŸ”é“", "ç©ºæ‰‹é“", "å‰‘é“", "ä¸–ç•Œæ¯", "äºšæ´²æ¯", "äºšè¿ä¼š", "è”èµ›", "ä¿±ä¹éƒ¨", "çƒé˜Ÿ", "é‡‘ç‰Œ", "é“¶ç‰Œ", "é“œç‰Œ"],
        "ç§‘æŠ€": ["ç§‘æŠ€", "ç§‘å­¦", "AI", "äº’è”ç½‘", "æ‰‹æœº", "èŠ¯ç‰‡", "èˆªå¤©", "ç ”å‘", "åŠå¯¼ä½“", "äººå·¥æ™ºèƒ½", "æœºå™¨äºº", "æ— äººæœº", "5G", "6G", "å«æ˜Ÿ", "ç«ç®­", "æ¢æµ‹", "å®‡å®™", "å¤ªç©º", "ç”Ÿç‰©", "åŸºå› ", "ç–«è‹—", "è¯ç‰©", "ç™Œç—‡", "è¯ºè´å°”", "ç‰©ç†", "åŒ–å­¦", "æ•°å­¦", "å¤©æ–‡", "é»‘æ´", "é‡å­", "è¶…å¯¼", "ææ–™", "ç”µæ± ", "èƒ½æº", "æ¸…æ´", "ç¯ä¿", "è‡ªåŠ¨é©¾é©¶", "å…ƒå®‡å®™", "åŒºå—é“¾", "åŠ å¯†è´§å¸", "æ¯”ç‰¹å¸"],
        "å¨±ä¹": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ˜æ˜Ÿ", "åŠ¨æ¼«", "æ¸¸æˆ", "åŠ¨ç”»", "æ¼«ç”»", "å£°ä¼˜", "å¶åƒ", "æ­Œæ‰‹", "æ¼”å‘˜", "å¯¼æ¼”", "å‰§é›†", "æ—¥å‰§", "éŸ©å‰§", "ç»¼è‰º", "æ¼”å”±ä¼š", "ç¥¨æˆ¿", "æ¦œå•", "çº¢ç™½", "å‰åœåŠ›", "å®«å´éª", "æ–°æµ·è¯š", "é¬¼ç­", "å’’æœ¯", "æµ·è´¼ç‹", "ç«å½±", "æŸ¯å—", "å®å¯æ¢¦", "é©¬é‡Œå¥¥", "å¡å°”è¾¾"]
    }

    for category, words in keywords.items():
        if any(word in title for word in words):
            return category
    return "å…¶ä»–"

def fetch_google_china_news():
    print("æ­£åœ¨æŠ“å– Google Newsï¼ˆæ—¥æœ¬ç‰ˆ Â· ä¸­å›½ç›¸å…³ï¼‰...")
    # æœç´¢è¿‡å» 24 å°æ—¶ ('when:1d') çš„æ–°é—»ï¼Œä»¥é˜²ä¸­é—´æ¼æŠ“ã€‚
    url = "https://news.google.com/rss/search?q=ä¸­å›½+when:1d&hl=ja&gl=JP&ceid=JP:ja" 
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        print(f"æŠ“åˆ° {len(feed.entries)} æ¡åŸå§‹æ¡ç›®")
        return feed.entries
    except Exception as e:
        print(f"æŠ“å–å¤±è´¥: {e}")
        return []

def process_entries(entries):
    processed_data = []
    translator = GoogleTranslator(source='ja', target='zh-CN')

    for entry in entries:
        title = entry.title
        link = entry.link
        published = entry.published_parsed
        
        # è¿‡æ»¤æ‰åŒ…å«"æ ª"ï¼ˆè‚¡ç¥¨ï¼‰ç­‰æ— å…³ä¿¡æ¯çš„æ ‡é¢˜
        if "æ ª" in title or "å¸‚åœº" in title or "é“­æŸ„" in title:
             # å†æ¬¡ç¡®è®¤æ˜¯å¦çœŸçš„æ˜¯çº¯è´¢ç»è¡Œæƒ…ï¼Œå¦‚æœåŒ…å«é‡è¦æ”¿æ²»å…³é”®è¯åˆ™ä¿ç•™
            if not any(k in title for k in ["ä¹ è¿‘å¹³", "é¦–ç›¸", "å¤–äº¤", "å†›äº‹"]):
                continue

        # ç¿»è¯‘æ ‡é¢˜
        try:
            title_zh = translator.translate(title)
        except:
            title_zh = title

        # æå–æ‘˜è¦å¹¶æˆªæ–­
        summary = ""
        if 'summary' in entry:
            # æ¸…ç† HTML æ ‡ç­¾
            soup = BeautifulSoup(entry.summary, 'html.parser')
            text = soup.get_text()
            # ç®€å•æˆªæ–­ (å…ˆå–å‰150å­—ç¿»è¯‘ï¼Œå†æˆªæ–­åˆ°50å­—)
            try:
                summary_zh = translator.translate(text[:150])
                if len(summary_zh) > 50:
                    summary = summary_zh[:50] + "..."
                else:
                    summary = summary_zh
            except:
                summary = text[:50] + "..."
        
        # æå–å›¾ç‰‡
        image_url = extract_image(entry)
        
        # æ ¼å¼åŒ–æ—¶é—´
        timestamp = time.mktime(published)
        time_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(timestamp))
        
        # åˆ†ç±»
        category = classify_news(title_zh)

        processed_data.append({
            "title": title_zh,
            "link": link,
            "image": image_url,
            "summary": summary,
            "category": category,
            "time_str": time_str,
            "timestamp": timestamp,
            "origin": entry.source.title if 'source' in entry else "Google News"
        })
    
    return processed_data

def update_news():
    entries = fetch_google_china_news()
    new_data = process_entries(entries)
    
    # å­˜æ¡£ä»Šæ—¥æ•°æ®
    archive_dir = "archive"
    os.makedirs(archive_dir, exist_ok=True)
    today_str = get_current_jst_time().strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{today_str}.json")

    # è¯»å–å·²æœ‰ä»Šæ—¥æ•°æ®å¹¶å»é‡åˆå¹¶
    final_today_list = []
    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                final_today_list = json.load(f)
        except:
            pass

    existing_links = {i['link'] for i in final_today_list}
    new_count = 0 # ç»Ÿè®¡æ–°å¢æ¡ç›®
    
    for item in new_data:
        if item['link'] not in existing_links:
            # æ–°å‘ç°çš„æ–‡ç« ç›´æ¥æ’å…¥åˆ°åˆ—è¡¨å¤´éƒ¨
            final_today_list.insert(0, item)
            new_count += 1 

    # -------------------------------------------------------------
    # ğŸŒŸ å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶æŒ‰ç…§æ—¶é—´æˆ³å€’åºæ’åº
    # ç¡®ä¿æœ€æ–°æ–°é—»ï¼ˆæ—¶é—´æˆ³æœ€å¤§çš„ï¼‰æ€»åœ¨åˆ—è¡¨æœ€å‰é¢ã€‚
    final_today_list.sort(key=lambda x: x['timestamp'], reverse=True)
    # -------------------------------------------------------------
    
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(final_today_list, f, ensure_ascii=False, indent=2)

    # èšåˆè¿‘30å¤©ç”Ÿæˆé¦–é¡µ data.json
    home_data = []
    seen_links = set()
    today = get_current_jst_time()

    for i in range(30):
        date = (today - datetime.timedelta(days=i)).strftime("%Y-%m-%d")
        path = os.path.join(archive_dir, f"{date}.json")
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    day_data = json.load(f)
                    for item in day_data:
                        if item['link'] not in seen_links:
                            home_data.append(item)
                            seen_links.add(item['link'])
            except:
                pass

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(home_data, f, ensure_ascii=False, indent=2)

    # ğŸŒŸ æ”¹è¿›ï¼šæ‰“å°å‘ç°çš„æ–°é—»æ•°é‡ï¼Œæ–¹ä¾¿åœ¨ Actions æ—¥å¿—ä¸­ç¡®è®¤
    print(f"å‘ç°äº† {new_count} æ¡æ–°æ–‡ç« ã€‚")
    print(f"æ›´æ–°å®Œæˆï¼ä»Šæ—¥ {len(final_today_list)} æ¡ï¼Œé¦–é¡µå…± {len(home_data)} æ¡æ–°é—»")

if __name__ == "__main__":
    update_news()