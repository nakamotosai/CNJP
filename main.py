import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time
import requests
from bs4 import BeautifulSoup

# è®¾ç½®æ—¶åŒº UTC+9
JST_OFFSET = datetime.timedelta(hours=9)

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

def extract_image(entry):
    # å°è¯•æå–å›¾ç‰‡çš„é€»è¾‘ (Google News ä¸“ç”¨)
    content_html = ""
    if 'summary' in entry:
        content_html = entry.summary
    elif 'description' in entry:
        content_html = entry.description
    
    if content_html:
        try:
            soup = BeautifulSoup(content_html, 'html.parser')
            img = soup.find('img')
            if img and 'src' in img.attrs:
                return img['src']
        except:
            pass
    return ""

def fetch_google_china_news():
    print("ğŸš€ æ­£åœ¨æŠ“å– Google News (æ—¥æœ¬/ä¸­å›½ç›¸å…³)...")
    # å…³é”®è¯ï¼šä¸­å›½
    # ceid=JP:ja é™åˆ¶ä¸ºæ—¥æœ¬ç‰ˆ
    # when:1d é™åˆ¶è¿‡å»24å°æ—¶ (æˆ‘ä»¬æ¯å¤©å­˜ï¼Œé¦–é¡µèšåˆ7å¤©ï¼Œæ‰€ä»¥æŠ“24å°æ—¶å¤Ÿäº†)
    url = "https://news.google.com/rss/search?q=ä¸­å›½+when:1d&hl=ja&gl=JP&ceid=JP:ja"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        if response.status_code != 200:
            return []
        feed = feedparser.parse(response.content)
        return feed.entries
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        return []

def process_entries(entries):
    processed = []
    translator = GoogleTranslator(source='auto', target='zh-CN')
    
    # æ¯æ¬¡æœ€å¤šæŠ“ 30 æ¡
    for entry in entries[:30]:
        original_title = entry.title
        # å»æ‰åª’ä½“åç¼€ (ä¾‹å¦‚ " - NHK NEWS")
        clean_title = original_title.split(' - ')[0]
        
        try:
            zh_title = translator.translate(clean_title)
        except:
            zh_title = clean_title 

        image_url = extract_image(entry)
        
        # è·å–å½“å‰æ—¶é—´å¯¹è±¡
        now = get_current_jst_time()
        
        # å°è¯•è§£æ RSS è‡ªå¸¦çš„æ—¶é—´
        try:
            if hasattr(entry, 'published_parsed'):
                # entry.published_parsed æ˜¯ UTC æ—¶é—´ï¼Œéœ€è½¬ä¸º JST
                pub_tm = entry.published_parsed
                dt_utc = datetime.datetime(*pub_tm[:6])
                dt_jst = dt_utc + datetime.timedelta(hours=9)
            else:
                dt_jst = now
        except:
            dt_jst = now

        # æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²
        time_display = dt_jst.strftime("%m-%d %H:%M") # æ˜¾ç¤ºä¸º 11-28 10:00
        timestamp = dt_jst.timestamp() # ç”¨äºæ’åºçš„æ•°å­—

        item = {
            "title": zh_title,
            "origin": original_title,
            "link": entry.link,
            "time_str": time_display,
            "timestamp": timestamp, # æ’åºç”¨
            "image": image_url
        }
        processed.append(item)
        time.sleep(0.2)
        
    return processed

def update_news():
    # 1. æŠ“å–ä»Šæ—¥æœ€æ–°
    raw_entries = fetch_google_china_news()
    new_data = process_entries(raw_entries)

    # 2. å­˜å…¥ä»Šæ—¥å­˜æ¡£
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
    
    today = get_current_jst_time()
    today_str = today.strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{today_str}.json")
    
    # è¯»å–æ—§çš„ä»Šæ—¥å­˜æ¡£ï¼ˆåˆå¹¶å»é‡ï¼‰
    final_today_list = []
    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                final_today_list = json.load(f)
        except:
            pass

    # åˆå¹¶é€»è¾‘
    existing_links = set(i['link'] for i in final_today_list)
    for item in new_data:
        if item['link'] not in existing_links:
            final_today_list.insert(0, item) # æ–°çš„æ”¾å‰é¢
    
    # ä¿å­˜ä»Šæ—¥å­˜æ¡£
    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(final_today_list, f, ensure_ascii=False, indent=2)
    print(f"âœ… ä»Šæ—¥å­˜æ¡£æ›´æ–° ({len(final_today_list)}æ¡)")

    # 3. ç”Ÿæˆé¦–é¡µæ•°æ® (èšåˆè¿‡å» 30 å¤©)
    print("ğŸ”„ æ­£åœ¨èšåˆè¿‘ 30 å¤©æ•°æ®...")
    home_data = []
    seen_links = set()

    # å€’åºéå†è¿‡å» 30 å¤© (ä»Šå¤© -> 30å¤©å‰)
    for i in range(30):
        target_date = today - datetime.timedelta(days=i)
        d_str = target_date.strftime("%Y-%m-%d")
        f_path = os.path.join(archive_dir, f"{d_str}.json")
        
        if os.path.exists(f_path):
            try:
                with open(f_path, 'r', encoding='utf-8') as f:
                    day_data = json.load(f)
                    for item in day_data:
                        if item['link'] not in seen_links:
                            home_data.append(item)
                            seen_links.add(item['link'])
            except:
                pass
    
    # é»˜è®¤æŒ‰çƒ­åº¦/RSSé¡ºåºä¿ç•™ (æˆ–è€…æŒ‰æ—¶é—´æ’ï¼Œè¿™é‡Œå…ˆä¿æŒRSSåŸåºï¼Œå‰ç«¯è´Ÿè´£æ’åº)
    # Google RSS æœ¬èº«å°±æ˜¯æŒ‰â€œç›¸å…³æ€§/çƒ­åº¦â€æ’åºçš„
    
    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(home_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… data.json æ›´æ–°å®Œæ¯• (åŒ…å« {len(home_data)} æ¡æ–°é—»)")

if __name__ == "__main__":
    update_news()