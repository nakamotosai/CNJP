import feedparser
from deep_translator import GoogleTranslator
import json
import os
import datetime
import time
import requests
from bs4 import BeautifulSoup # ç”¨æ¥ä» Google æ–°é—»é‡Œæ‰£å›¾ç‰‡

# è®¾ç½®æ—¶åŒº UTC+9
JST_OFFSET = datetime.timedelta(hours=9)

# Google æ–°é—» (æ—¥æœ¬ç„¦ç‚¹) RSS
RSS_URL = "https://news.google.com/rss?hl=ja&gl=JP&ceid=JP:ja"

def get_current_jst_time():
    return datetime.datetime.utcnow() + JST_OFFSET

def update_news():
    print("ğŸš€ å¼€å§‹æŠ“å– Google æ–°é—»(æ—¥æœ¬çƒ­æ¦œ)...")
    
    # ä¼ªè£…å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(RSS_URL, headers=headers, timeout=10)
        print(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code != 200:
            print("âŒ è®¿é—®è¢«æ‹’ç»")
            return

        feed = feedparser.parse(response.content)
        
    except Exception as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return

    if not feed.entries:
        print("âš ï¸ æœªè·å–åˆ°æ–°é—»")
        return

    translator = GoogleTranslator(source='auto', target='zh-CN')
    
    archive_dir = "archive"
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
        
    date_str = get_current_jst_time().strftime("%Y-%m-%d")
    archive_path = os.path.join(archive_dir, f"{date_str}.json")
    
    existing_links = set()
    current_archive_data = []

    # è¯»å–æ—§æ•°æ®
    if os.path.exists(archive_path):
        try:
            with open(archive_path, 'r', encoding='utf-8') as f:
                current_archive_data = json.load(f)
                for item in current_archive_data:
                    existing_links.add(item['link'])
        except:
            pass

    new_items_count = 0
    
    # æŠ“å–å‰ 20 æ¡
    for entry in feed.entries[:20]:
        link = entry.link
        if link in existing_links:
            continue

        # ç¿»è¯‘æ ‡é¢˜
        # Googleæ–°é—»æ ‡é¢˜é€šå¸¸æ˜¯ "æ ‡é¢˜ - åª’ä½“å"ï¼Œæˆ‘ä»¬åªç¿»è¯‘æ¨ªæ å‰é¢çš„éƒ¨åˆ†ä¼šæ›´å‡†ç¡®
        clean_title = entry.title.split(' - ')[0]
        try:
            zh_title = translator.translate(clean_title)
        except:
            zh_title = clean_title
        
        # --- ğŸ”¥ æ ¸å¿ƒï¼šä» Google æè¿°ä¸­æå–å›¾ç‰‡ ---
        image_url = ""
        if 'summary' in entry:
            # Google æŠŠå›¾ç‰‡æ”¾åœ¨ summary çš„ html æ ‡ç­¾é‡Œ
            soup = BeautifulSoup(entry.summary, 'html.parser')
            img_tag = soup.find('img')
            if img_tag and 'src' in img_tag.attrs:
                image_url = img_tag['src']
        # ---------------------------------------
        
        time_str = get_current_jst_time().strftime("%H:%M")

        item_data = {
            "title": zh_title,
            "origin": entry.title,
            "link": link,
            "time": time_str,
            "image": image_url
        }
        
        current_archive_data.insert(0, item_data)
        existing_links.add(link)
        new_items_count += 1
        time.sleep(0.5)

    print(f"âœ… æ–°å¢äº† {new_items_count} æ¡æ–°é—»")

    with open(archive_path, 'w', encoding='utf-8') as f:
        json.dump(current_archive_data, f, ensure_ascii=False, indent=2)

    with open('data.json', 'w', encoding='utf-8') as f:
        json.dump(current_archive_data[:20], f, ensure_ascii=False, indent=2)
    print("âœ… data.json æ›´æ–°æˆåŠŸ")

if __name__ == "__main__":
    update_news()